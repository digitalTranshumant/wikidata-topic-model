{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Wikidata Items for topic classification  \n",
    "### based on https://github.com/geohci/wikidata-topic-model-api\n",
    "\n",
    "This notebook make usage of the [WMF's Hadoop Cluster](https://wikitech.wikimedia.org/wiki/Analytics/Systems/Cluster). If you don't have access to that cluster, you will need to rewrite the code using the [Wikidata Dump](https://dumps.wikimedia.org/wikidata). \n",
    "\n",
    "Here we use two tables from the [Wikimedia Data Lake](https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake):\n",
    "\n",
    "* [wmf.wikidata_item_page_link](https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Wikidata_item_page_link): Containig the relation between Wikidata Items and Page Titles. This is results are equivalent to the 'sitelinks' value that you will find in the Wikidata Dump.\n",
    "\n",
    "* [wmf.wikidata_entity](https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Wikidata_entity): From we exract the claims for each Wikidata Items. You will find equilivant information in the claims field of Wikidata dump. \n",
    "\n",
    "This code works is based on the  [wikidata-topic-model-api](https://github.com/geohci/wikidata-topic-model-api). If want to get the topic for sinlge (or small set of) Wikidata Item(s), we recommend you to use this experimental API: https://tools.wmflabs.org/wiki-topic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          partition|\n",
      "+-------------------+\n",
      "|snapshot=2020-01-06|\n",
      "|snapshot=2020-01-13|\n",
      "|snapshot=2020-01-20|\n",
      "|snapshot=2020-01-27|\n",
      "|snapshot=2020-02-03|\n",
      "|snapshot=2020-02-10|\n",
      "|snapshot=2020-02-17|\n",
      "|snapshot=2020-02-24|\n",
      "|snapshot=2020-03-02|\n",
      "|snapshot=2020-03-23|\n",
      "|snapshot=2020-03-30|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check partitions on  wikidata_item_page_link \n",
    "spark.sql(''' \n",
    " show partitions wmf.wikidata_item_page_link \n",
    " ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all Wikidata Items with a page in at least one project\n",
    "\n",
    "QwithPage = spark.sql('''SELECT \n",
    "item_id as wikidata_item\n",
    "FROM wmf.wikidata_item_page_link  \n",
    "WHERE snapshot=\"2020-03-30\" AND page_namespace=0 GROUP BY wikidata_item\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract direct claims for Wikidata Items on wikidata_entity. \n",
    "directClaims = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  subject,\n",
    "  claim.mainSnak.property AS predicate,\n",
    "  claim.mainSnak.dataValue.value AS object,\n",
    "  -- needed to correctly interpret the object value\n",
    "  claim.mainSnak.dataValue.typ AS object_typ\n",
    "FROM (\n",
    "  SELECT\n",
    "    id as subject,\n",
    "    explode(claims) as claim\n",
    "  FROM wmf.wikidata_entity\n",
    "  WHERE snapshot = '2020-03-02' \n",
    ") t  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restrict to Wikidata Items with sitelink (pages existing on the )\n",
    "directClaimsWithPages = directClaims.join(QwithPage,directClaims['subject']==QwithPage['wikidata_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directClaimsWithPages.createTempView('directClaimsWithPages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to claims to 'wikibase-entityid'. That means that we create Triples Qx Py Qz,\n",
    "#excluding other kind of relations such as Qx Py 'string'\n",
    "\n",
    "claims  = spark.sql('''SELECT subject, predicate, object\n",
    "                    FROM  directClaimsWithPages WHERE  object_typ = 'wikibase-entityid'\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+\n",
      "| subject|predicate|              object|\n",
      "+--------+---------+--------------------+\n",
      "|Q1000211|    P1313|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P17|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P31|{\"entity-type\":\"i...|\n",
      "|Q1000211|     P131|{\"entity-type\":\"i...|\n",
      "|Q1000211|     P131|{\"entity-type\":\"i...|\n",
      "|Q1000211|     P131|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P47|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P47|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P47|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P47|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P47|{\"entity-type\":\"i...|\n",
      "|Q1000211|      P47|{\"entity-type\":\"i...|\n",
      "|Q1000211|     P166|{\"entity-type\":\"i...|\n",
      "|Q1000919|      P31|{\"entity-type\":\"i...|\n",
      "|Q1000919|     P112|{\"entity-type\":\"i...|\n",
      "|Q1001340|      P31|{\"entity-type\":\"i...|\n",
      "|Q1001340|     P106|{\"entity-type\":\"i...|\n",
      "|Q1001340|      P20|{\"entity-type\":\"i...|\n",
      "|Q1001340|     P735|{\"entity-type\":\"i...|\n",
      "|Q1001340|      P21|{\"entity-type\":\"i...|\n",
      "+--------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "claims.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We parse the claims, converting to python dictionary and filtering out all claims with an id\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def getId(obj):\n",
    "    try:\n",
    "        d =  eval(obj)\n",
    "        return d.get('id')\n",
    "    except:\n",
    "        return 'Nothing'\n",
    "claims = claims.withColumn('id', getId(col(\"object\")))\n",
    "claimsWithId = claims.where(claims['id'] != 'Nothing').select('subject','predicate','id')\n",
    "claimsWithId = claimsWithId.withColumn('predicateToObject', concat(col(\"predicate\"), lit(\" \"), col(\"id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that we have exploded the claims (meaning that now for each claim we have a row)\n",
    "# We group all claims by subject, concatenating all the claims, to create the Bag of Words that is the input\n",
    "# of the wikidata-topic-model\n",
    "import pyspark.sql.functions as f\n",
    "allClaimsPerItem = claimsWithId.groupby('subject').agg(f.concat_ws(\" \", f.collect_list(claimsWithId.predicateToObject)).alias('BoW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  subject|                 BoW|\n",
      "+---------+--------------------+\n",
      "| Q1000211|P1313 Q65191356 P...|\n",
      "| Q1000919|P31 Q43229 P112 Q...|\n",
      "| Q1001340|P31 Q5 P106 Q4260...|\n",
      "| Q1001788|P1454 Q134161 P31...|\n",
      "| Q1001929|P1435 Q19558910 P...|\n",
      "| Q1002146|P31 Q1748957 P279...|\n",
      "| Q1002610|        P31 Q4167410|\n",
      "|Q10026565|P31 Q3464665 P179...|\n",
      "| Q1003019|P1435 Q19558910 P...|\n",
      "| Q1003138|P31 Q4830453 P159...|\n",
      "| Q1003174|P361 Q1747183 P17...|\n",
      "| Q1003185|P31 Q11173 P31 Q2...|\n",
      "| Q1003336|P1313 Q65182521 P...|\n",
      "| Q1003686|P1465 Q32665933 P...|\n",
      "| Q1003747|P641 Q41466 P2348...|\n",
      "| Q1004179|P611 Q846687 P31 ...|\n",
      "|  Q100433|P734 Q26736379 P3...|\n",
      "| Q1004468|P31 Q5 P106 Q3618...|\n",
      "|  Q100453|P734 Q14947294 P3...|\n",
      "|  Q100477|P641 Q2736 P54 Q4...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allClaimsPerItem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save results in CSV in HDFS\n",
    "allClaimsPerItem.write.csv('claimsPerWikidaItemTopicInputAllWikidataItems.csv',header=True,mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/11 00:28:28 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\r\n"
     ]
    }
   ],
   "source": [
    "# Because we want to use allClaimsPerItem data in a Python Kernel without access to Hadoop\n",
    "# we take the data out from the cluster, and filter (to remove the repetition of CSV headers generated \n",
    "# in the by the way hadoop fs -text works)\n",
    "!hadoop fs -text  claimsPerWikidaItemTopicInputAllWikidataItems.csv/* > claimsPerWikidaItemTopicInputAllWikidataItems.csv.tmp\n",
    "!awk 'BEGIN{f=\"\"}{if($0!=f){print $0}if(NR==1){f=$0}}' claimsPerWikidaItemTopicInputAllWikidataItems.csv.tmp > claimsPerWikidaItemTopicInputAllWikidataItems.csv\n",
    "!rm claimsPerWikidaItemTopicInputAllWikidataItems.csv.tmp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
